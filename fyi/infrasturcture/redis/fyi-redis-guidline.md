# Redis å¯¦ä½œæŒ‡å—

## ğŸ¯ Redis æ ¸å¿ƒç”¨é€”

### 1. **è¨˜æ†¶é«”è³‡æ–™åº«**
Redis æ˜¯ä¸€å€‹é–‹æºçš„è¨˜æ†¶é«”è³‡æ–™çµæ§‹å„²å­˜ç³»çµ±ï¼Œä¸»è¦ç”¨æ–¼å¿«å–ã€æœƒè©±ç®¡ç†å’Œå³æ™‚è³‡æ–™è™•ç†ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- **é«˜æ€§èƒ½**: åŸºæ–¼è¨˜æ†¶é«”æ“ä½œï¼Œé€Ÿåº¦æ¥µå¿«
- **è³‡æ–™çµæ§‹è±å¯Œ**: æ”¯æ´å­—ä¸²ã€åˆ—è¡¨ã€é›†åˆã€æœ‰åºé›†åˆã€é›œæ¹Šç­‰
- **æŒä¹…åŒ–**: æ”¯æ´ RDB å’Œ AOF å…©ç¨®æŒä¹…åŒ–æ–¹å¼
- **åˆ†æ•£å¼**: æ”¯æ´ä¸»å¾è¤‡è£½å’Œå¢é›†æ¨¡å¼

### 2. **ä¸»è¦ä½¿ç”¨å ´æ™¯**
```python
# å¿«å–
redis_client.setex("user:123", 300, json.dumps(user_data))

# è¨ˆæ•¸å™¨
redis_client.incr("post:456:likes")

# æœƒè©±ç®¡ç†
redis_client.setex(f"session:{session_id}", 3600, user_id)

# é™æµ
redis_client.incr(f"rate_limit:{user_id}:api")
```

## ğŸ”„ èˆ‡è¨‚é–±è€…æ¨¡å¼çš„é—œä¿‚

### **Redis Pub/Sub å¯¦ç¾è¨‚é–±è€…æ¨¡å¼**

#### 1. **ç™¼å¸ƒ-è¨‚é–±æ©Ÿåˆ¶**
```python
# ç™¼å¸ƒè€…ï¼ˆPublisherï¼‰
def publish_post_created(post_id, author_id):
    message = {
        "event": "post_created",
        "post_id": post_id,
        "author_id": author_id,
        "timestamp": datetime.now().isoformat()
    }
    redis_client.publish("post_events", json.dumps(message))

# è¨‚é–±è€…ï¼ˆSubscriberï¼‰
def subscribe_to_post_events():
    pubsub = redis_client.pubsub()
    pubsub.subscribe("post_events")
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            data = json.loads(message['data'])
            handle_post_event(data)

def handle_post_event(data):
    if data['event'] == 'post_created':
        # é€šçŸ¥é—œæ³¨è€…
        notify_followers(data['author_id'], data['post_id'])
        # æ›´æ–°æ¨è–¦ç³»çµ±
        update_recommendations(data['post_id'])
        # æ›´æ–°æœå°‹ç´¢å¼•
        update_search_index(data['post_id'])
```

#### 2. **å¤šé »é“è¨‚é–±**
```python
# è¨‚é–±å¤šå€‹äº‹ä»¶é »é“
def subscribe_to_multiple_channels():
    pubsub = redis_client.pubsub()
    pubsub.subscribe("post_events", "user_events", "comment_events")
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            channel = message['channel'].decode()
            data = json.loads(message['data'])
            
            if channel == "post_events":
                handle_post_event(data)
            elif channel == "user_events":
                handle_user_event(data)
            elif channel == "comment_events":
                handle_comment_event(data)
```

## ğŸš€ åœ¨ Threads å°ˆæ¡ˆä¸­çš„å¯¦ä½œå»ºè­°

### **ç¬¬ä¸€éšæ®µï¼šåŸºç¤å¿«å–ç­–ç•¥**

#### 1. **ç”¨æˆ¶è³‡æ–™å¿«å–**
```python
# cache.py
class UserCache:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.cache_ttl = 300  # 5åˆ†é˜
    
    def get_user(self, user_id):
        cache_key = f"user:{user_id}"
        cached_data = self.redis.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        
        # å¾è³‡æ–™åº«æŸ¥è©¢
        user = User.objects.get(id=user_id)
        user_data = {
            "id": user.id,
            "username": user.username,
            "email": user.email,
            "followers_count": user.followers_count,
            "followings_count": user.followings_count,
            "posts_count": user.posts_count
        }
        
        # å¿«å–è³‡æ–™
        self.redis.setex(cache_key, self.cache_ttl, json.dumps(user_data))
        return user_data
    
    def invalidate_user(self, user_id):
        cache_key = f"user:{user_id}"
        self.redis.delete(cache_key)
```

#### 2. **è²¼æ–‡åˆ—è¡¨å¿«å–**
```python
class PostCache:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.cache_ttl = 60  # 1åˆ†é˜
    
    def get_hot_posts(self, user_id, limit=20):
        cache_key = f"hot_posts:{user_id}:{limit}"
        cached_posts = self.redis.get(cache_key)
        
        if cached_posts:
            return json.loads(cached_posts)
        
        # å¾è³‡æ–™åº«æŸ¥è©¢ç†±é–€è²¼æ–‡
        posts = Post.objects.filter(
            created_at__gte=timezone.now() - timedelta(days=7)
        ).order_by('-likes_count')[:limit]
        
        posts_data = [self._serialize_post(post) for post in posts]
        
        # å¿«å–è³‡æ–™
        self.redis.setex(cache_key, self.cache_ttl, json.dumps(posts_data))
        return posts_data
    
    def invalidate_hot_posts(self):
        # æ¸…é™¤æ‰€æœ‰ç†±é–€è²¼æ–‡å¿«å–
        for key in self.redis.scan_iter(match="hot_posts:*"):
            self.redis.delete(key)
```

### **ç¬¬äºŒéšæ®µï¼šå³æ™‚åŠŸèƒ½å¯¦ç¾**

#### 1. **å³æ™‚è¨ˆæ•¸å™¨**
```python
class RealTimeCounters:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def increment_likes(self, content_id, content_type):
        key = f"{content_type}:{content_id}:likes"
        self.redis.incr(key)
        
        # ç™¼å¸ƒäº‹ä»¶
        self.publish_counter_update(content_id, content_type, "likes")
    
    def decrement_likes(self, content_id, content_type):
        key = f"{content_type}:{content_id}:likes"
        self.redis.decr(key)
        
        # ç™¼å¸ƒäº‹ä»¶
        self.publish_counter_update(content_id, content_type, "likes")
    
    def publish_counter_update(self, content_id, content_type, counter_type):
        message = {
            "event": "counter_update",
            "content_id": content_id,
            "content_type": content_type,
            "counter_type": counter_type,
            "value": self.redis.get(f"{content_type}:{content_id}:{counter_type}")
        }
        self.redis.publish("counter_events", json.dumps(message))
```

#### 2. **ç·šä¸Šç”¨æˆ¶è¿½è¹¤**
```python
class OnlineUsersTracker:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def user_online(self, user_id):
        # ä½¿ç”¨æœ‰åºé›†åˆè¿½è¹¤ç·šä¸Šç”¨æˆ¶
        self.redis.zadd("online_users", {user_id: time.time()})
        
        # ç™¼å¸ƒç”¨æˆ¶ä¸Šç·šäº‹ä»¶
        self.publish_user_status(user_id, "online")
    
    def user_offline(self, user_id):
        # ç§»é™¤ç·šä¸Šç”¨æˆ¶
        self.redis.zrem("online_users", user_id)
        
        # ç™¼å¸ƒç”¨æˆ¶ä¸‹ç·šäº‹ä»¶
        self.publish_user_status(user_id, "offline")
    
    def get_online_users(self, limit=100):
        # ç²å–æœ€è¿‘æ´»èºçš„ç”¨æˆ¶
        now = time.time()
        online_users = self.redis.zrevrangebyscore(
            "online_users", now, now - 300, 0, limit
        )
        return [int(user_id) for user_id in online_users]
    
    def publish_user_status(self, user_id, status):
        message = {
            "event": "user_status_change",
            "user_id": user_id,
            "status": status,
            "timestamp": time.time()
        }
        self.redis.publish("user_events", json.dumps(message))
```

### **ç¬¬ä¸‰éšæ®µï¼šé€²éšåŠŸèƒ½**

#### 1. **API é™æµ**
```python
class RateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def is_allowed(self, user_id, endpoint, limit=100, window=3600):
        key = f"rate_limit:{user_id}:{endpoint}"
        current = self.redis.incr(key)
        
        if current == 1:
            self.redis.expire(key, window)
        
        return current <= limit
    
    def get_remaining_requests(self, user_id, endpoint, limit=100):
        key = f"rate_limit:{user_id}:{endpoint}"
        current = int(self.redis.get(key) or 0)
        return max(0, limit - current)
```

#### 2. **åˆ†æ•£å¼é–**
```python
class DistributedLock:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def acquire_lock(self, lock_name, timeout=10):
        identifier = str(uuid.uuid4())
        end = time.time() + timeout
        
        while time.time() < end:
            if self.redis.set(f"lock:{lock_name}", identifier, nx=True, ex=timeout):
                return identifier
            time.sleep(0.001)
        
        return False
    
    def release_lock(self, lock_name, identifier):
        pipe = self.redis.pipeline(True)
        
        while True:
            try:
                pipe.watch(f"lock:{lock_name}")
                if pipe.get(f"lock:{lock_name}") == identifier:
                    pipe.multi()
                    pipe.delete(f"lock:{lock_name}")
                    pipe.execute()
                    return True
                pipe.unwatch()
                break
            except redis.WatchError:
                pass
        
        return False
```

## âš™ï¸ Redis é…ç½®å»ºè­°

### **Docker Compose é…ç½®**
```yaml
# docker-compose.yml
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru

volumes:
  redis_data:
```

### **Django è¨­å®š**
```python
# settings.py
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# Redis é€£æ¥æ± 
import redis
from django.conf import settings

redis_pool = redis.ConnectionPool.from_url(settings.REDIS_URL)
redis_client = redis.Redis(connection_pool=redis_pool)
```

### **å¿«å–é…ç½®**
```python
# settings.py
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': REDIS_URL,
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
        }
    }
}

# æœƒè©±å„²å­˜
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
SESSION_CACHE_ALIAS = 'default'
```

## ğŸ“Š ç›£æ§å’Œç¶­è­·

### **Redis ç›£æ§**
```python
class RedisMonitor:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def get_memory_usage(self):
        info = self.redis.info('memory')
        return {
            'used_memory': info['used_memory'],
            'used_memory_human': info['used_memory_human'],
            'maxmemory': info.get('maxmemory', 0)
        }
    
    def get_key_count(self):
        return self.redis.dbsize()
    
    def get_slow_queries(self):
        return self.redis.slowlog_get(10)
    
    def clear_expired_keys(self):
        return self.redis.execute_command('MEMORY', 'PURGE')
```

### **å¿«å–çµ±è¨ˆ**
```python
class CacheStats:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def get_hit_rate(self):
        info = self.redis.info('stats')
        hits = info['keyspace_hits']
        misses = info['keyspace_misses']
        total = hits + misses
        
        if total == 0:
            return 0
        
        return hits / total
    
    def get_top_keys(self, pattern="*", limit=10):
        keys = []
        for key in self.redis.scan_iter(match=pattern):
            ttl = self.redis.ttl(key)
            keys.append({
                'key': key.decode(),
                'ttl': ttl,
                'type': self.redis.type(key).decode()
            })
        
        return sorted(keys, key=lambda x: x['ttl'], reverse=True)[:limit]
```

## ğŸ¯ æœ€ä½³å¯¦è¸

### **1. å¿«å–ç­–ç•¥**
- **Cache-Aside**: æ‡‰ç”¨ç¨‹å¼è² è²¬å¿«å–ç®¡ç†
- **Write-Through**: åŒæ™‚å¯«å…¥å¿«å–å’Œè³‡æ–™åº«
- **Write-Behind**: å…ˆå¯«å¿«å–ï¼Œå¾Œå¯«è³‡æ–™åº«

### **2. è¨˜æ†¶é«”ç®¡ç†**
- è¨­å®šé©ç•¶çš„ `maxmemory` é™åˆ¶
- ä½¿ç”¨ `allkeys-lru` æˆ– `volatile-lru` ç­–ç•¥
- å®šæœŸç›£æ§è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³

### **3. è³‡æ–™ä¸€è‡´æ€§**
- ä½¿ç”¨åˆ†æ•£å¼é–ç¢ºä¿åŸå­æ“ä½œ
- å¯¦ç¾å¿«å–å¤±æ•ˆç­–ç•¥
- ä½¿ç”¨ç‰ˆæœ¬è™Ÿè™•ç†ä¸¦ç™¼æ›´æ–°

### **4. æ•ˆèƒ½å„ªåŒ–**
- ä½¿ç”¨ Pipeline æ‰¹é‡æ“ä½œ
- é¿å…å¤§éµå€¼å°
- åˆç†è¨­å®šéæœŸæ™‚é–“

### **5. å®‰å…¨è€ƒé‡**
- è¨­å®šå¯†ç¢¼èªè­‰
- é™åˆ¶ç¶²è·¯è¨ªå•
- å®šæœŸå‚™ä»½é‡è¦è³‡æ–™
